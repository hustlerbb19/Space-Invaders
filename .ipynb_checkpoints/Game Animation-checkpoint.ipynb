{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gym\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.losses import huber_loss\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Conv2D, Dense, Flatten\n",
    "from matplotlib import animation\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "import os\n",
    "\n",
    "# Set randomization defaults for solution reproducability\n",
    "np.random.seed(21)\n",
    "rn.seed(21)\n",
    "tf.set_random_seed(21)\n",
    "\n",
    "# Set visualization defaults\n",
    "sns.set_context('notebook')\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "%matplotlib inline \n",
    "\n",
    "# Set input directory from model training, ouput director for storing visualizations\n",
    "INPUT_DIR = 'DDQN_model_output/'\n",
    "OUTPUT_DIR = 'Assets/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare `ReplayMemory` and `DDQNAgent` classes (mimicing earlier code); define image processing and animation helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function for preprocessing mimage\n",
    "def process_image(obs):\n",
    "    img = obs[12:195:2, ::2] # Trim image to playable display domain, downsample\n",
    "    img = img.mean(axis = 2) # Convert image to greyscale\n",
    "    img = img.astype(np.uint8)  # Cast pixel values as integers\n",
    "    img = img.reshape((1,) + img.shape + (1,)) # Reshape image to 4D array of dimensions 1 x H x L x 1 for Keras compatibility\n",
    "    return img # return processed image array\n",
    "\n",
    "# Declare Replay buffer python class\n",
    "class ReplayMemory:\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = int(maxlen)\n",
    "        self.buf = np.empty(shape = self.maxlen, dtype = np.object)\n",
    "        self.index = 0\n",
    "        self.length = 0\n",
    "        \n",
    "    def append(self, data): # Define method for storing samples in replay memory buffer\n",
    "        self.buf[self.index] = data\n",
    "        self.length = min(self.length + 1, self.maxlen)\n",
    "        self.index = (self.index + 1) % self.maxlen # If `maxlen` is exceeded, replace prior memory samples with new observations\n",
    "    \n",
    "    def sample(self, batch_size, with_replacement = True): # Define method for randomly sampling replay memory; default to sampling with replacement (faster than alternative)\n",
    "        if with_replacement:\n",
    "            indices = np.random.randint(self.length, size = batch_size)\n",
    "        else:\n",
    "            indices = np.random.permutation(self.length)[:batch_size]\n",
    "        return self.buf[indices]\n",
    "\n",
    "# Define helper function for randomly sampling from memory buffer of class `ReplayMemory`\n",
    "def sample_memories(memory_store, batch_size):\n",
    "    cols = [[], [], [], [], []]\n",
    "    for memory in memory_store.sample(batch_size):\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1)\n",
    "\n",
    "# Declare Double Deep-Q Learning Network Agent class\n",
    "class DDQNAgent:\n",
    "    def __init__(self, image_shape, action_space, learning_rate, rho, optimizer_epsilon, replay_memory, gamma, epsilon_max, epsilon_min, epsilon_steps):\n",
    "        self.image_shape = image_shape\n",
    "        self.action_space = action_space\n",
    "        self.learning_rate = learning_rate\n",
    "        self.rho = rho\n",
    "        self.optimizer_epsilon = optimizer_epsilon\n",
    "        self.memory = replay_memory\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = (epsilon_max - epsilon_min)/epsilon_steps # set step size for decay from `epsilon_max` to `epsilon_min` over `epsilon_step` steps\n",
    "        self.ε = epsilon_max # set initial ε-greedy learning `ε` parameter value equal to `epsilon_max`\n",
    "        self.model, self.target_model = self._build_model() # instantiate core model, target model (same initial weights)\n",
    "    \n",
    "    def _build_model(self): # Define ANN architecture for Deep-Q Network Learning:\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Conv2D(32, 8, strides = 4, padding = \"valid\", activation = \"relu\", input_shape = self.image_shape, data_format = \"channels_last\"))\n",
    "        model.add(Conv2D(64, 4, strides = 2, padding = \"valid\", activation = \"relu\"))\n",
    "        model.add(Conv2D(64, 3, strides = 1, padding = \"valid\", activation = \"relu\"))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation = \"relu\"))\n",
    "        model.add(Dense(self.action_space))\n",
    "        \n",
    "        model.compile(loss = huber_loss, optimizer = RMSprop(lr = self.learning_rate, rho = self.rho, epsilon = self.optimizer_epsilon))\n",
    "        return model, model\n",
    "\n",
    "    def act(self, state): # Define ε-greedy learning action method\n",
    "        \n",
    "        ## Apply image processing helper function to current environment state; convert image pixel values to [0,1]\n",
    "        state_proc = process_image(state)\n",
    "        state_scaled = state_proc/255\n",
    "        \n",
    "        ## Perform action based on ε value: \n",
    "        ε = max(self.epsilon_min, self.ε)\n",
    "        \n",
    "        ### If Agent acting randomly, select random action from available action-space\n",
    "        if np.random.rand() < ε:\n",
    "            return rn.randrange(self.action_space)\n",
    "        \n",
    "        ### If Agent not acting randomly, choose action based on policy of selecting action yielding highest Q-value\n",
    "        Q_value = self.model.predict(state_scaled)[0] \n",
    "        return np.argmax(Q_value)\n",
    "         \n",
    "    def remember(self, state, action, reward, next_state, done): # Define method for appending agent experiences to memory buffer\n",
    "        self.memory.append((state, action, reward, next_state, done)) \n",
    "       \n",
    "    def train(self, batch_size): # Define method for Agent to traing DNN with experiences sampled from memory buffer\n",
    "        ## Extract minibatch samples from memory buffer\n",
    "        state, action, reward, next_state, done = sample_memories(self.memory, batch_size)\n",
    "        \n",
    "        ## Convert current, next state observation pixel values to [0,1]\n",
    "        state_scaled = state/255\n",
    "        next_state_scaled = next_state/255\n",
    "        \n",
    "        ## Update best Q value for each observation within minibatch\n",
    "        for state_scaled, action, reward, next_state_scaled, done in zip(state_scaled, action, reward, next_state_scaled, done):\n",
    "            \n",
    "            ### Predict Q-values for current state\n",
    "            Q_values = self.model.predict(state_scaled) \n",
    "            \n",
    "            ### Update Q-value for selected action\n",
    "            target = reward ### If training episode is complete, set Q-value for selected action equal to present reward\n",
    "            if not done: ### If episode is ongoing, set Q-value for selected action discounted future Q-value predicted using target model\n",
    "                target = reward + self.gamma * np.amax(self.target_model.predict(next_state_scaled)[0]) \n",
    "    \n",
    "            Q_values[0][action] = target\n",
    "            \n",
    "        ## Fit online model on minibatch to optimize quality of Q-value prediction for selected action; update ε-greedy learning parameter value\n",
    "        history = self.model.fit(state_scaled, Q_values, batch_size = batch_size, epochs = 1, verbose = 0)\n",
    "        \n",
    "        if self.ε > self.epsilon_min:\n",
    "            self.ε -= self.epsilon_decay\n",
    "            \n",
    "        return history\n",
    "    \n",
    "    def update_target_model(self): # Define method to copy online DQN model feature weights to target model\n",
    "        model_weights = self.model.get_weights()\n",
    "        self.target_model.set_weights(model_weights)\n",
    "        \n",
    "    def save(self, name): # Define method for saving model weights\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "    def load(self, name): # Define method for loading saved model weights\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "# Define helper function for animating game history\n",
    "def display_frames_as_gif(frame_history, filename_gif = None):\n",
    "    \n",
    "    plt.figure(figsize = (frame_history[0].shape[1] / 72.0, frame_history[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frame_history[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frame_history[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frame_history), interval = 50)\n",
    "    if filename_gif: \n",
    "        anim.save(filename_gif, writer = 'imagemagick', fps = 20)\n",
    "    display(display_animation(anim, default_mode = 'loop'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate DDQN Agent; load weights from trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set DQN Agent parameters\n",
    "## Set RL environment parameters\n",
    "INPUT_SHAPE = (92, 80, 1)\n",
    "ACTION_SPACE = 6\n",
    "\n",
    "# Set RMSProp optimizer learning parameters\n",
    "LEARNING_RATE = 0.00025\n",
    "RHO = 0.95\n",
    "OPTIMIZER_EPSILON = 0.01\n",
    "\n",
    "## Set Q-learning γ discount-rate parameter\n",
    "GAMMA = 0.99\n",
    "\n",
    "## Set replay memory size parameter; instantiate memory buffer\n",
    "REPLAY_MEMORY_SIZE = 6 * 10**4\n",
    "REPLAY_MEMORY = ReplayMemory(REPLAY_MEMORY_SIZE)\n",
    "\n",
    "## Set ε-greedy learning parameters for actor-agent\n",
    "EPSILON_MAX = 1\n",
    "EPSILON_MIN = 0.1\n",
    "DECAY_STEPS = 2 * 10**4\n",
    "\n",
    "# Instantiate Deep-Q Learning Agent; load weights from previously trained trial\n",
    "agent = DDQNAgent(INPUT_SHAPE, ACTION_SPACE, LEARNING_RATE, RHO, OPTIMIZER_EPSILON, REPLAY_MEMORY, GAMMA, EPSILON_MIN, EPSILON_MIN, DECAY_STEPS)\n",
    "agent.load(INPUT_DIR + 'weights_100K_frames.hdf5')\n",
    "agent.ε = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play one game of ATARI 2600 Space Invaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set game defaults\n",
    "env = gym.make('SpaceInvaders-v0') # Instantiate Space Invaders gym environemtn\n",
    "state = env.reset() # Initialize environment\n",
    "done = False\n",
    "\n",
    "frameshistory = [state]\n",
    "\n",
    "# Loop over game instance\n",
    "while not done:\n",
    "    \n",
    "    env.render() # Render environment\n",
    "    \n",
    "    # Act within environment, remember outcome, update state variable\n",
    "    action = agent.act(state)\n",
    "    next_state, _, done, _ = env.step(action)\n",
    "    \n",
    "    frameshistory.append(next_state)\n",
    "    state = next_state\n",
    "\n",
    "env.close()\n",
    "end_time = datetime.now()\n",
    "\n",
    "# Report game length, save reult\n",
    "display_frames_as_gif(frameshistory, OUTPUT_DIR + 'game_run.gif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
